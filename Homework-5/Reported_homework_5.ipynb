{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, I'm Jobert Gutierrez and hereafter you'll find the logic and code used to answer the fifth assignment in the program Data Engineering Zoomcamp offered by Data Talks Club."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Module 5 Homework: Batch processing__\n",
    "\n",
    "In this homework we'll put what we learned about Spark in practice.\n",
    "\n",
    "For this homework we will be using the Yellow 2024-10 data from the official website:\n",
    "\n",
    "> wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
    "\n",
    "### __Question 1.Install Spark and PySpark__\n",
    "- Install Spark\n",
    "- Run PySpark\n",
    "- Create a local spark session\n",
    "- Execute spark.version.\n",
    "- What's the output?\n",
    "\n",
    "Note: \n",
    "\n",
    "To install PySpark follow this [guide](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/pyspark.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "Using the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The version of pyspark installed is __'3.3.2'__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Question 2.Yellow October 2024__\n",
    "Read the October 2024 Yellow into a Spark Dataframe.\n",
    "\n",
    "Repartition the Dataframe to 4 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n",
    "\n",
    "- 6MB\n",
    "- 25MB\n",
    "- 75MB\n",
    "- 100MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Answer: \n",
    "I first read the file for yellow trips in octobert 2024 as *!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-09.parquet* and the read into spark with the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"homework\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\n",
    "\n",
    "df = spark.read.parquet('yellow_tripdata_2024-10.parquet')\n",
    "\n",
    "df = df.repartition(4)\n",
    "\n",
    "df.write.parquet('yellow/2024/10/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the file is partitioned into 4 elements of __24.2 MB__ each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Question 3.Count records__\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October.\n",
    "\n",
    "- 85,567\n",
    "- 105,567\n",
    "- 125,567\n",
    "- 145,567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "Then, I read again the partitions and printed the schemas as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet('yellow/2024/10/')\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The changed the types of the columns wrongly interpreted accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField('VendorID', types.IntegerType(), True),\n",
    "    types.StructField('tpep_pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('tpep_dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('passenger_count', types.LongType(), True),\n",
    "    types.StructField('trip_distance', types.DoubleType(), True),\n",
    "    types.StructField('RatecodeID', types.LongType(), True),\n",
    "    types.StructField('store_and_fwd_flag', types.StringType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('payment_type', types.IntegerType(), True),\n",
    "    types.StructField('fare_amount', types.DoubleType(), True),\n",
    "    types.StructField('extra', types.DoubleType(), True),\n",
    "    types.StructField('mta_tax', types.DoubleType(), True),\n",
    "    types.StructField('tip_amount', types.DoubleType(), True),\n",
    "    types.StructField('tolls_amount', types.DoubleType(), True),\n",
    "    types.StructField('improvement_surcharge', types.DoubleType(), True),\n",
    "    types.StructField('total_amount', types.DoubleType(), True),\n",
    "    types.StructField('congestion_surcharge', types.DoubleType(), True),\n",
    "    types.StructField('Airport_fee', types.DoubleType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .schema(schema).parquet('yellow/2024/10/')\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df.withColumn('tpep_pickup_date', F.to_date(df.tpep_pickup_datetime) )\n",
    "\n",
    "df.filter(df.tpep_pickup_date == '2024-10-15').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the number of trips in october 15 is __128.893 trips__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Question 4.Longest trip__\n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?\n",
    "\n",
    "- 122\n",
    "- 142\n",
    "- 162\n",
    "- 182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "To get the longest trip in hours I prepared the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('yellow')\n",
    "\n",
    "df_result = spark.sql(\"\"\"\n",
    "    select PULocationID, DOLocationID, ((unix_timestamp(tpep_dropoff_datetime) - unix_timestamp(tpep_pickup_datetime))/3600) as duration \n",
    "    from yellow\n",
    "    where \n",
    "        tpep_dropoff_date >= '2024-10-01' and\n",
    "        tpep_dropoff_date < '2024-11-01' and\n",
    "        tpep_pickup_date >= '2024-10-01' and\n",
    "        tpep_pickup_date < '2024-11-01'\n",
    "    order by duration desc\n",
    "    limit 3\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Then, the longest trip took __162.61 hours__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Question 5.User Interface__\n",
    "\n",
    "Spark’s User Interface which shows the application's dashboard runs on which local port?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "The port for Spark’s User Interface is 4040."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Question 6.Least frequent pickup location zone__\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark:\n",
    "\n",
    "> wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv\n",
    "\n",
    "Using the zone lookup data and the Yellow October 2024 data, what is the name of the LEAST frequent pickup location Zone?\n",
    "\n",
    "- Governor's Island/Ellis Island/Liberty Island\n",
    "- Arden Heights\n",
    "- Rikers Island\n",
    "- Jamaica Bay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "Once I got the data from the link as *!wget https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv* then I read the data and created a temporary table as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "zones = spark.read.csv('taxi_zone_lookup.csv', header = \"True\")\n",
    "\n",
    "zones.createOrReplaceTempView('zones')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I used sparkSQL to answer the question as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    PUlocationID, zone, count(*) as count\n",
    "FROM\n",
    "    yellow\n",
    "INNER JOIN\n",
    "    zones\n",
    "ON \n",
    "    PULocationID == LocationID\n",
    "GROUP BY \n",
    "    PUlocationID, Zone\n",
    "ORDER BY\n",
    "    count\n",
    "LIMIT 7\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the least frequent pick-up location is __Governor's Island/Ellis Island/Liberty Island__."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
